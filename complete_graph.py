#!/usr/bin/env python3
"""
è„šæœ¬ï¼šè¡¥å…¨å›¾æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰è¿æ¥
ç›®æ ‡ï¼šè®© bay_vio_data_03_19.csv ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨å›¾ä¸­æœ‰è¾¹è¿æ¥
"""

import pandas as pd
import numpy as np
import os
from itertools import combinations
import random

def analyze_existing_data():
    """åˆ†æç°æœ‰æ•°æ®çš„èŠ‚ç‚¹èŒƒå›´"""
    print("=== åˆ†æç°æœ‰æ•°æ® ===")
    
    # åˆ†æè¯·æ±‚æ•°æ®ä¸­çš„èŠ‚ç‚¹
    vio_data = pd.read_csv('/Users/akiyama/Downloads/budgted_reinforcement_learning_for_fairness_in_real_time_dispatching_system-main/data/bay_vio_data_03_19.csv')
    
    origins = set()
    destinations = set()
    
    for _, row in vio_data.iterrows():
        if isinstance(row['street_marker'], str) and row['street_marker'].startswith('A'):
            origins.add(int(row['street_marker'][1:]))
        if isinstance(row['aim_marker'], str) and row['aim_marker'].startswith('A'):
            destinations.add(int(row['aim_marker'][1:]))
    
    print(f"è¯·æ±‚æ•°æ®ä¸­çš„èµ·å§‹èŠ‚ç‚¹èŒƒå›´: {min(origins)} - {max(origins)} (å…±{len(origins)}ä¸ª)")
    print(f"è¯·æ±‚æ•°æ®ä¸­çš„ç»ˆç‚¹èŠ‚ç‚¹èŒƒå›´: {min(destinations)} - {max(destinations)} (å…±{len(destinations)}ä¸ª)")
    
    # åˆ†æç°æœ‰å›¾æ•°æ®
    graph_data = pd.read_csv('/Users/akiyama/Downloads/budgted_reinforcement_learning_for_fairness_in_real_time_dispatching_system-main/data/dis_CBD_twoPs_03_19.csv')
    
    graph_nodes = set()
    for _, row in graph_data.iterrows():
        nodes = row['twoPs'].split('_')
        for node in nodes:
            if node.startswith('A'):
                graph_nodes.add(int(node[1:]))
    
    print(f"ç°æœ‰å›¾æ•°æ®ä¸­çš„èŠ‚ç‚¹èŒƒå›´: {min(graph_nodes)} - {max(graph_nodes)} (å…±{len(graph_nodes)}ä¸ª)")
    
    all_needed_nodes = origins.union(destinations)
    missing_nodes = all_needed_nodes - graph_nodes
    
    print(f"ç¼ºå¤±çš„èŠ‚ç‚¹: {len(missing_nodes)}ä¸ª")
    if len(missing_nodes) < 50:
        print(f"ç¼ºå¤±èŠ‚ç‚¹åˆ—è¡¨: {sorted(missing_nodes)}")
    else:
        print(f"éƒ¨åˆ†ç¼ºå¤±èŠ‚ç‚¹: {sorted(list(missing_nodes))[:20]}...")
    
    return all_needed_nodes, graph_nodes, missing_nodes

def generate_distances_from_existing():
    """ä»ç°æœ‰æ•°æ®ä¸­åˆ†æè·ç¦»åˆ†å¸ƒï¼Œç”¨äºç”Ÿæˆæ–°çš„è·ç¦»"""
    graph_data = pd.read_csv('/Users/akiyama/Downloads/budgted_reinforcement_learning_for_fairness_in_real_time_dispatching_system-main/data/dis_CBD_twoPs_03_19.csv')
    
    # æ’é™¤è‡ªç¯ï¼ˆA0_A0è¿™ç§ï¼‰
    non_zero_distances = graph_data[graph_data['distance'] > 0]['distance'].values
    
    stats = {
        'mean': np.mean(non_zero_distances),
        'std': np.std(non_zero_distances),
        'min': np.min(non_zero_distances),
        'max': np.max(non_zero_distances),
        'median': np.median(non_zero_distances)
    }
    
    print(f"\n=== ç°æœ‰è·ç¦»ç»Ÿè®¡ ===")
    print(f"å¹³å‡è·ç¦»: {stats['mean']:.2f}")
    print(f"æ ‡å‡†å·®: {stats['std']:.2f}")
    print(f"æœ€å°è·ç¦»: {stats['min']:.2f}")
    print(f"æœ€å¤§è·ç¦»: {stats['max']:.2f}")
    print(f"ä¸­ä½æ•°: {stats['median']:.2f}")
    
    return stats

def generate_realistic_distance(node1, node2, distance_stats):
    """ç”Ÿæˆè¾ƒä¸ºç°å®çš„è·ç¦»å€¼"""
    if node1 == node2:
        return 0
    
    # æ ¹æ®èŠ‚ç‚¹å·®å€¼ç”ŸæˆåŸºç¡€è·ç¦»
    node_diff = abs(node1 - node2)
    
    if node_diff == 1:
        # ç›¸é‚»èŠ‚ç‚¹ï¼Œè·ç¦»è¾ƒå°
        base_distance = np.random.normal(500, 200)
    elif node_diff <= 10:
        # è¿‘é‚»èŠ‚ç‚¹
        base_distance = np.random.normal(1500, 500)
    elif node_diff <= 50:
        # ä¸­ç­‰è·ç¦»
        base_distance = np.random.normal(3000, 1000)
    else:
        # è¿œè·ç¦»èŠ‚ç‚¹
        base_distance = np.random.normal(distance_stats['mean'], distance_stats['std'])
    
    # ç¡®ä¿è·ç¦»åœ¨åˆç†èŒƒå›´å†…
    distance = max(100, min(15000, abs(base_distance)))
    
    return round(distance, 2)

def create_complete_graph():
    """åˆ›å»ºå®Œæ•´çš„å›¾æ•°æ®"""
    print("\n=== å¼€å§‹ç”Ÿæˆå®Œæ•´å›¾æ•°æ® ===")
    
    all_needed_nodes, existing_nodes, missing_nodes = analyze_existing_data()
    distance_stats = generate_distances_from_existing()
    
    # è¯»å–ç°æœ‰å›¾æ•°æ®
    existing_graph = pd.read_csv('/Users/akiyama/Downloads/budgted_reinforcement_learning_for_fairness_in_real_time_dispatching_system-main/data/dis_CBD_twoPs_03_19.csv')
    
    # å­˜å‚¨æ‰€æœ‰è¾¹
    edges = {}
    
    # æ·»åŠ ç°æœ‰è¾¹
    for _, row in existing_graph.iterrows():
        edges[row['twoPs']] = row['distance']
    
    print(f"ç°æœ‰è¾¹æ•°é‡: {len(edges)}")
    
    # ä¸ºæ‰€æœ‰éœ€è¦çš„èŠ‚ç‚¹ç”Ÿæˆè¿æ¥
    all_nodes = sorted(list(all_needed_nodes))
    
    print(f"éœ€è¦ç¡®ä¿è¿é€šçš„èŠ‚ç‚¹æ€»æ•°: {len(all_nodes)}")
    
    # 1. ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘ä¸å‡ ä¸ªå…¶ä»–èŠ‚ç‚¹ç›¸è¿ï¼ˆä¿è¯è¿é€šæ€§ï¼‰
    print("ç”ŸæˆåŸºæœ¬è¿é€šæ€§...")
    for i, node in enumerate(all_nodes):
        # æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘è¿æ¥åˆ°å‰åå‡ ä¸ªèŠ‚ç‚¹
        connect_to = []
        
        # è¿æ¥åˆ°å‰å2ä¸ªèŠ‚ç‚¹
        if i > 0:
            connect_to.append(all_nodes[i-1])
        if i < len(all_nodes) - 1:
            connect_to.append(all_nodes[i+1])
        
        # éšæœºè¿æ¥åˆ°å‡ ä¸ªå…¶ä»–èŠ‚ç‚¹
        other_nodes = [n for n in all_nodes if n != node and n not in connect_to]
        if other_nodes:
            random.shuffle(other_nodes)
            connect_to.extend(other_nodes[:min(3, len(other_nodes))])
        
        # ç”Ÿæˆè¾¹
        for target in connect_to:
            edge1 = f"A{node}_A{target}"
            edge2 = f"A{target}_A{node}"
            
            if edge1 not in edges and edge2 not in edges:
                distance = generate_realistic_distance(node, target, distance_stats)
                edges[edge1] = distance
                edges[edge2] = distance
    
    # 2. æ·»åŠ è‡ªç¯ï¼ˆA0_A0 = 0ï¼‰
    print("æ·»åŠ è‡ªç¯...")
    for node in all_nodes:
        self_edge = f"A{node}_A{node}"
        if self_edge not in edges:
            edges[self_edge] = 0
    
    # 3. ä¸ºé«˜é¢‘èŠ‚ç‚¹å¯¹æ·»åŠ ç›´æ¥è¿æ¥
    print("æ·»åŠ é«˜é¢‘èŠ‚ç‚¹å¯¹è¿æ¥...")
    vio_data = pd.read_csv('/Users/akiyama/Downloads/budgted_reinforcement_learning_for_fairness_in_real_time_dispatching_system-main/data/bay_vio_data_03_19.csv')
    
    # ç¡®ä¿æ‰€æœ‰è¯·æ±‚ä¸­çš„èŠ‚ç‚¹å¯¹éƒ½æœ‰è¾¹
    print("ç¡®ä¿æ‰€æœ‰è¯·æ±‚çš„èŠ‚ç‚¹å¯¹éƒ½æœ‰è¿æ¥...")
    for _, row in vio_data.iterrows():
        if (isinstance(row['street_marker'], str) and row['street_marker'].startswith('A') and
            isinstance(row['aim_marker'], str) and row['aim_marker'].startswith('A')):
            
            origin = int(row['street_marker'][1:])
            dest = int(row['aim_marker'][1:])
            
            edge1 = f"A{origin}_A{dest}"
            edge2 = f"A{dest}_A{origin}"
            
            # å¦‚æœè¿™ä¸ªèŠ‚ç‚¹å¯¹æ²¡æœ‰è¿æ¥ï¼Œå°±æ·»åŠ 
            if edge1 not in edges and edge2 not in edges:
                distance = generate_realistic_distance(origin, dest, distance_stats)
                edges[edge1] = distance
                # æ³¨æ„ï¼šè¿™é‡Œä¸æ·»åŠ åå‘è¾¹ï¼Œå› ä¸ºå›¾å¯èƒ½æ˜¯æœ‰å‘çš„
    
    print(f"æœ€ç»ˆå›¾ä¸­çš„è¾¹æ•°é‡: {len(edges)}")
    
    # åˆ›å»ºæ–°çš„DataFrame
    new_edges_data = []
    for edge_name, distance in edges.items():
        new_edges_data.append({'distance': distance, 'twoPs': edge_name})
    
    new_graph_df = pd.DataFrame(new_edges_data)
    new_graph_df = new_graph_df.sort_values('twoPs').reset_index(drop=True)
    
    # ä¿å­˜åˆ°æ–°æ–‡ä»¶
    output_file = '/Users/akiyama/Downloads/budgted_reinforcement_learning_for_fairness_in_real_time_dispatching_system-main/data/dis_CBD_twoPs_03_19_complete.csv'
    new_graph_df.to_csv(output_file, index=False)
    
    print(f"å®Œæ•´å›¾æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    # éªŒè¯è¦†ç›–æ€§
    verify_coverage(output_file)
    
    return output_file

def verify_coverage(graph_file):
    """éªŒè¯å›¾æ˜¯å¦è¦†ç›–æ‰€æœ‰éœ€è¦çš„èŠ‚ç‚¹å¯¹"""
    print("\n=== éªŒè¯å›¾è¦†ç›–æ€§ ===")
    
    # è¯»å–è¯·æ±‚æ•°æ®
    vio_data = pd.read_csv('/Users/akiyama/Downloads/budgted_reinforcement_learning_for_fairness_in_real_time_dispatching_system-main/data/bay_vio_data_03_19.csv')
    
    # è¯»å–å›¾æ•°æ®
    graph_data = pd.read_csv(graph_file)
    
    # æå–å›¾ä¸­çš„æ‰€æœ‰è¾¹
    graph_edges = set()
    for _, row in graph_data.iterrows():
        graph_edges.add(row['twoPs'])
    
    # æ£€æŸ¥è¯·æ±‚æ•°æ®ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹å¯¹æ˜¯å¦éƒ½åœ¨å›¾ä¸­
    missing_edges = []
    total_requests = 0
    
    for _, row in vio_data.iterrows():
        if (isinstance(row['street_marker'], str) and row['street_marker'].startswith('A') and
            isinstance(row['aim_marker'], str) and row['aim_marker'].startswith('A')):
            
            origin = row['street_marker']
            dest = row['aim_marker']
            edge1 = f"{origin}_{dest}"
            edge2 = f"{dest}_{origin}"
            
            total_requests += 1
            
            if edge1 not in graph_edges and edge2 not in graph_edges:
                missing_edges.append((origin, dest))
    
    print(f"æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"ç¼ºå¤±çš„è¾¹: {len(missing_edges)}")
    
    if missing_edges:
        print("éƒ¨åˆ†ç¼ºå¤±çš„è¾¹:")
        for i, (o, d) in enumerate(missing_edges[:10]):
            print(f"  {o} -> {d}")
        if len(missing_edges) > 10:
            print(f"  ... è¿˜æœ‰ {len(missing_edges) - 10} ä¸ªç¼ºå¤±è¾¹")
        return False
    else:
        print("âœ… æ‰€æœ‰è¯·æ±‚çš„èŠ‚ç‚¹å¯¹éƒ½åœ¨å›¾ä¸­æœ‰å¯¹åº”çš„è¾¹ï¼")
        return True

def update_utils_py(new_graph_file):
    """æ›´æ–°utils.pyä»¥ä½¿ç”¨æ–°çš„å®Œæ•´å›¾æ–‡ä»¶"""
    print(f"\n=== æ›´æ–°utils.pyä»¥ä½¿ç”¨æ–°å›¾æ–‡ä»¶ ===")
    
    # å¤‡ä»½åŸæ–‡ä»¶
    utils_file = '/Users/akiyama/Downloads/budgted_reinforcement_learning_for_fairness_in_real_time_dispatching_system-main/data/utils.py'
    backup_file = utils_file + '.backup'
    
    with open(utils_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    with open(backup_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"å·²å¤‡ä»½åŸæ–‡ä»¶åˆ°: {backup_file}")
    
    # æ›´æ–°æ–‡ä»¶è·¯å¾„
    new_filename = os.path.basename(new_graph_file)
    updated_content = content.replace(
        "file_path = os.path.join(current_dir, 'dis_CBD_twoPs_03_19.csv')",
        f"file_path = os.path.join(current_dir, '{new_filename}')"
    )
    
    with open(utils_file, 'w', encoding='utf-8') as f:
        f.write(updated_content)
    
    print(f"å·²æ›´æ–°utils.pyä½¿ç”¨æ–°å›¾æ–‡ä»¶: {new_filename}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è¡¥å…¨å›¾æ•°æ®...")
    
    # è®¾ç½®éšæœºç§å­ä»¥è·å¾—å¯é‡å¤çš„ç»“æœ
    random.seed(42)
    np.random.seed(42)
    
    try:
        # åˆ›å»ºå®Œæ•´å›¾
        new_graph_file = create_complete_graph()
        
        # æ›´æ–°utils.py
        update_utils_py(new_graph_file)
        
        print("\nğŸ‰ å›¾æ•°æ®è¡¥å…¨å®Œæˆï¼")
        print("ç°åœ¨å¯ä»¥è¿è¡Œå®éªŒäº†:")
        print("PYTHONPATH=. python train/train.py --select 0")
        print("PYTHONPATH=. python train/train.py --select 1")
        
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()